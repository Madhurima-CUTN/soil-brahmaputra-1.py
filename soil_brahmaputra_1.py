# -*- coding: utf-8 -*-
"""soil-brahmaputra-1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N_E7uObjENQj7WcrgbHsCcu-T16LL3z3
"""

from google.colab import drive
drive.mount('/content/drive')

from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np
import tensorflow as tf
import numpy as np
import matplotlib
import cv2
import keras
from matplotlib import pyplot as plt
from keras.models import Sequential
from keras.optimizers import *
from keras.callbacks import ModelCheckpoint
from keras.layers import *
from keras.preprocessing.image import ImageDataGenerator

batch_size = 25
img_height = 256
img_width = 256

data_dir='/content/drive/My Drive/soildata/exp4'

train_ds = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.40,
  subset="training",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)

val_ds = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.40,
  subset="validation",
  seed=123,
  image_size=(img_height, img_width),
  batch_size=batch_size)

class_names = train_ds.class_names
print(class_names)

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 10))
for images, labels in train_ds.take(1):
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(images[i].numpy().astype("uint8"))
    plt.title(class_names[labels[i]])
    plt.axis("off")

for image_batch, labels_batch in train_ds:
  print(image_batch.shape)
  print(labels_batch.shape)
  break

for image_batch, labels_batch in val_ds:
  print(image_batch.shape)
  print(labels_batch.shape)
  break

AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

from tensorflow.keras import layers
normalization_layer = layers.Rescaling(1./255)

normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))
image_batch, labels_batch = next(iter(normalized_ds))
first_image = image_batch[0]
# Notice the pixel values are now in `[0,1]`.
print(np.min(first_image), np.max(first_image))

num_classes = len(class_names)

model = Sequential([
  layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),
  layers.Conv2D(16, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(32, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(64, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(16, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Flatten(),
  #layers.Dense(128, activation='relu'),
  layers.Dense(num_classes)
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model.summary()

from keras.utils import plot_model
plot_model(model, to_file='model.png',show_shapes=True,show_layer_names=True)

epochs=100
history = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs
)

filters, bias = model.layers[1].get_weights()
f_min, f_max = filters.min(), filters.max()
filters = (filters - f_min) / (f_max - f_min)
n_filters = 16
ix=1
fig = plt.figure(figsize=(8,8))
for i in range(n_filters):
    # get the filters
    f = filters[:,:,:,i]
    for j in range(3):
        # subplot for 6 filters and 3 channels
        plt.subplot(8,6,ix)
        plt.imshow(f[:,:,j] ,cmap='gray')
        ix+=1
#plot the filters
plt.show()

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

from sklearn.metrics import classification_report,confusion_matrix
import numpy as np
batch_size=25
images_list = []
labels_list = []
for images, labels in val_ds.take(1):
    images_list.append(images)
    labels_list.append(labels.numpy())
    print("labels_list:")
    print(labels_list)
    predictions = np.argmax(model.predict(images_list), axis=-1)
    predictions = predictions.reshape(1,-1)[0]
    print(np.unique(labels_list, return_counts=True))
    print(classification_report(labels_list[0], predictions, target_names = ['(Clay)','(Sandy Clay)','(Silty)']))
    print(confusion_matrix(labels_list[0],predictions))

data_augmentation = keras.Sequential(
  [
    layers.RandomFlip("horizontal", input_shape=(img_height, img_width, 3)),
    layers.RandomRotation(0.9),
  #  layers.RandomZoom(0.2, 0.2),
   # layers.RandomCrop(128, 128),
   # layers.RandomTranslation(0.3,0.3),
    layers.GaussianNoise(0.2),
  ]
)

model2 = Sequential([
 #layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),
  data_augmentation,
  layers.Conv2D(16, 3, activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(32, 3,  activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(64, 3, activation='relu'),
  layers.MaxPooling2D(),
  layers.Flatten(),
  layers.Dense(32, activation='relu'),
  layers.Dense(num_classes)
])

model2.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model2.summary()

from keras.utils import plot_model
plot_model(model2, to_file='model2.png',show_shapes=True,show_layer_names=True)

epochs = 100
history2 = model2.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs
)

acc2 = history2.history['accuracy']
val_acc2 = history2.history['val_accuracy']

loss2 = history2.history['loss']
val_loss2 = history2.history['val_loss']

epochs_range2 = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range2, acc2, label='Training Accuracy')
plt.plot(epochs_range2, val_acc2, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range2, loss2, label='Training Loss')
plt.plot(epochs_range2, val_loss2, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

plt.plot(history2.history['loss'])
plt.plot(history2.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()

from sklearn.metrics import classification_report,confusion_matrix
import numpy as np

images_list = []
labels_list = []
for images, labels in val_ds.take(1):
    images_list.append(images)
    labels_list.append(labels.numpy())

predictions = np.argmax(model.predict(images_list), axis=-1)
predictions = predictions.reshape(1,-1)[0]
print("labels_list:")
print(labels_list)
print("numpy unique counts:")
print(np.unique(labels_list, return_counts=True))

print(classification_report(labels_list[0], predictions, target_names = ['1 (Clay)','2 (Sandy Clay)','3 (Silty)']))
print(confusion_matrix(labels_list[0],predictions))

from sklearn.metrics import classification_report,confusion_matrix
import numpy as np

images_list2 = []
labels_list2 = []
for images, labels in val_ds.take(1):
    images_list2.append(images)
    labels_list2.append(labels.numpy())

predictions2 = np.argmax(model2.predict(images_list), axis=-1)
predictions = predictions.reshape(1,-1)[0]
print("labels_list:")
print(labels_list)
print("numpy unique counts:")
print(np.unique(labels_list2, return_counts=True))

print(classification_report(labels_list2[0], predictions2, target_names = ['1 (Clay)','2 (Sandy Clay)','3 (Silty)']))
print(confusion_matrix(labels_list2[0],predictions2))

plt.figure(figsize=(10, 10))
for images, _ in train_ds.take(1):
  for i in range(9):
    augmented_images = data_augmentation(images)
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(augmented_images[0].numpy().astype("uint8"))
    plt.axis("off")

model3 = Sequential([
  data_augmentation,
#  layers.Rescaling(1./255),
  layers.Conv2D(16, 3, padding='same', activation='relu',kernel_regularizer = tf.keras.regularizers.l1(l=0.01)),
  layers.MaxPooling2D(),
  layers.Conv2D(32, 3, padding='same', activation='relu',kernel_regularizer = tf.keras.regularizers.l1(l=0.01)),
  layers.MaxPooling2D(),
  layers.Conv2D(64, 3, padding='same', activation='relu',kernel_regularizer = tf.keras.regularizers.l1(l=0.01)),
  layers.MaxPooling2D(),
  layers.Dropout(0.2),
  layers.Flatten(),
  layers.Dense(128, activation='relu'),
  layers.Dense(num_classes, name="outputs")
])

model3.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model3.summary()

from keras.utils import plot_model
plot_model(model3, to_file='model3.png',show_shapes=True,show_layer_names=True)

epochs = 100
history3 = model3.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs
)

acc3 = history3.history['accuracy']
val_acc3 = history3.history['val_accuracy']

loss3 = history3.history['loss']
val_loss3 = history3.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc3, label='Training Accuracy')
plt.plot(epochs_range, val_acc3, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss3, label='Training Loss')
plt.plot(epochs_range, val_loss3, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

model4 = Sequential([
  data_augmentation,
 # layers.Rescaling(1./255),
  layers.Conv2D(16, 3, padding='same', activation='relu',kernel_regularizer = tf.keras.regularizers.l2(l=0.01)),
  layers.MaxPooling2D(),
  layers.Conv2D(32, 3, padding='same', activation='relu',kernel_regularizer = tf.keras.regularizers.l2(l=0.01)),
  layers.MaxPooling2D(),
  layers.Conv2D(64, 3, padding='same', activation='relu',kernel_regularizer = tf.keras.regularizers.l2(l=0.01)),
  layers.MaxPooling2D(),
  layers.Dropout(0.2),
  layers.Flatten(),
#  layers.Dense(128, activation='relu'),
  layers.Dense(num_classes, name="outputs")
])

model4.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

epochs = 100
history4 = model4.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs
)

print(acc)
print(acc2)
print(acc3)
print(acc4)

